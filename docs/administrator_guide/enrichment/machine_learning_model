---
layout: default
title: Machine Learning Model
parent: Enrichment
grand_parent: Administrator Guide
nav_order: 4
---

# Machine Learning Model
{: .no_toc }


Machine Learning models can help detect risk or identify irrelevant content that shouldn't be alerted on to reduce false-positive alerting.
{: .fs-6 .fw-300 }

1. TOC
{:toc}

---

## Setting up a Machine Learning Model

Please see the Machine Learning Model in the [User Guide](mailto:https://relativitydev.github.io/relativity-trace-documentation/docs/user_guide/enrichment/)

### Naming Convention on Saved Searches
1. Each Machine Learning Model has to have a unique name. This unique name is used both in the SS created for the Rule, the Rule itself, and any fields created for that model.
2. Saved Seach is created once an Active Version is set

## Improving Model Accuracy

Data that has already been coded for the behaviors you are attempting to identify should be added to your models. 

1. Locate the positive or negative examples by finding communications at the level you are targetting
2. Use the Edit Mass Operation and the Machine Learning Document Layout to code the documents on the appropriate "Risk Detection Label" or "Irrelevant Content Detection Label" field
3. Once coded, these documents will automatically be pulled into the associated model as training documents
4. Once satisfied, build a new model version

## Validating Model Accuracy

You want to be confident that the model is accurately classifying communications based on its purpose. 

### Running a Validation Test

To run a Validation Test, we first need to identify a subset of analyzed documents to manually review for accuracy. We have a Relativity Script that tags these documents for review.

1. Run the `Trace Machine Learning Statistical Sample` Relativity Script
   1. Create a Saved Search called "ML [Model Purpose] Validation Test Population" (eg. `ML Spam Validation Test Population`) that includes all documents analyzed between the last time you ran a validation test and today
   2. Navigate to the `Scripts` tab
   3. Find the `Trace Machine Learning Statistical Sample` Relativity Script
   4. Select `Run` within the view
   5. For the `Saved Search` select the "ML [Model Purpose] Validation Test Population" (eg. `ML Spam Validation Test Population`) saved search
   6. For the `Analytics Set` select the Active Learning Project set associated with the model you are looking to validate
   7. For the  `Pending Validation Field` select the appropriate "ML [Model Purpose] Pending Validation" (eg. `ML Spam Pending Validation`)
   8. For the `Max Number of Documents to Review (default 300)` field keep the default, or populate it with a higher number if you want greater confidence in the Precision and Recall metrics
   9. Press the `Run` within the pop up

![image-20210413144648358](media/machine_learning/image-20210413144648358.png)

2. Review each document tagged by the `Trace Machine Learning Statistical Sample` for whether they are positive or negative examples of the behavior your model is attempting to identify
   1. Navigate to the "ML [Model Purpose] Validation Test Population" (eg. `ML Spam Validation Test Population`)
   2. Add a Search Condition to filter to only show documents where the "ML [Model Purpose] Pending Validation" (eg. `ML Spam Pending Validation`) field is set to `YES`
   3. Review each document for whether it is a Positive or Negative example of the behaviors in the model on the "ML [Model Purpose] Validation" (eg. `ML Spam Validation`) field
3. Run the `Trace Machine Learning Validation Test` Relativity Script
   1. Navigate to the `Scripts` tab
   2. Find the `Trace Machine Learning Validation Test` Relativity Script
   3. Select `Run` within the view
   4. For `Analytics Set` select the Active Learning Project set associated with the model you are looking to validate
   5. For `Decision Field` select the Yes/No "ML [Model Purpose] Validation" (eg. `ML Spam Validation`) field that you reviewed documents on associated with the model you are validating
   6. For `Saved Search` select the "ML [Model Purpose] Validation" (eg. `ML Spam Validation`) Saved Search associated with the model you are validating
   7. Press the `Run` within the pop up


![image-20210413143726362](media/machine_learning/image-20210413143726362.png)

### Interpreting Validation Test Results

The `Trace Machine Learning Validation Test` calculates Precision and Recall across all Rank Cutoff values allowing for you to understand the Machine Learning model's accuracy at different implementation ranks.

#### What is Precision and Recall and Rank Cutoffs?

To explain Precision and Recall, you first need to understand the four different types of document categorizations.

| Document Type  | Description                                                  |
| -------------- | ------------------------------------------------------------ |
| True Positive  | A True Positive is a document that the model believes IS an example of what you are attempting to identify, and the model is CORRECT that it is a Positive example. |
| True Negative  | A True Negative is a document that the model believes IS NOT an example of what you are attempting to identify, and the model is CORRECT that it is a Negative example. |
| False Positive | A False Positive is a document that the model believes IS an example of what you are attempting to identify, and the model is WRONG as the document is actually a Negative example. |
| False Negative | A False Negative is a document that the model believes IS NOT an example of what you are attempting to identify, and the model is WRONG as the document is actually a Positive example. |

Here is a matrix that helps explain the possible document categorizations:

|                                 | Is Actually POSITIVE | Is Actually NEGATIVE |
| ------------------------------- | -------------------- | -------------------- |
| **Model Predicted as POSITIVE** | `True Positive`      | `False Positive`     |
| **Model Predicted as NEGATIVE** | `False Negative`     | `True Negative`      |


**Precision**

Precision is a statistical representation of how right your model is at predicting a document was a Positive example. A value of 100 (1.0) means that the model was right 100% of the time when it marked a document as Positive.

> **Calculation:** Precision = `True Positive` / (`True Positive`+`False Positive`)
>
> **Example:** 0.90 (aka. 90) = 90/(90+10)
>
> **Explanation:** This means that your model is right 90% of the time when it believes a document is Positive

**Recall**

Recall is a statistical representation of how often your model misses Positive examples by thinking they are Negative. A value of 100 (1.0) means that the model was right 100% of the time when it marked a document as Negative.

>  **Calculation:** Recall = `True Positive` / (`True Positive`+`False Negative`)
>
> **Example:** 0.75 (aka. 75) = 90/(90+30)
>
> **Explanation:** This means that your model is right 90% of the time when it believes a document is Positive


**Rank Cutoff**

Rank Cutoff is the rank used by the model to classify a document as either Positive or Negative. With a model ranking every document between 0 and 100 it could specify 90 as the rank cutoff where documents with a Rank equal to or above 90 are Positive and a Rank below 90 is Negative. Or a model could specify 70 as the rank cutoff where documents with a Rank equal to or above 70 are Positive and a Rank below 70 is Negative. Adjusting the rank cutoff  allows for you to hone the model for it's specific use case.

> **Example:** 
> 1. If I'm creating a Spam model, I need to be almost 100% sure that the documents the model is identifying as Spam are actually Spam because I will remove these document from Alerting. This means I need my model to have extremely **high Precision** (98-100). In this scenario, I would set my Rank Cutoff to be extremely high (>80) so that my Precision is extremely high. By adjusting the system for high Precision, my Recall will drop. This means that there are many Spam documents in what the model thinks is not Spam. These Spam documents will still be alerted on and will show up to a reviewer as a false-positive alert. This is okay though, because we are erroring on the side of not removing content that could contain misconduct.
> 2. If I'm creating an Insider Information model, I want to make sure I'm casting a wide net and alerting on anything that could possibly be this type of misconduct. This means I need my model to have extremely **high Recall** (90-100). In this scenario, I would set my Rank Cutoff to be low (60-80) so that my Recall is extremely high. By adjusting the system for high Recall, my Precision will drop. This means that there are many non-Insider Information documents in what the model thinks is Insider Information. These non-Insider Information documents will be alerted on and will show up to a reviewer as a false-positive alert. This is okay though, because it ensures we don't miss any type of misconduct.

### Using Results to Remove Irrelevant Content (<u>Very Confident</u> in Accuracy)

Once you are very confident in your Active Learning models that identify irrelevant content, you are ready to use those results to actually remove those documents from being analyzed for alerts.

1. Locate the Saved Search used for the `Omit from Alert Rules` functionality within the `Rules Evaluation` Task (See Trace Document Flow Overview section for more information)
2. Update the Saved Search with "AND Machine Learning model rank is greater than *X*" (eg. AND `CSR - Spam Cat. Set::Category Rank` > `80`)
   1. Select the appropriate Rank Cutoff based on your Validation Test

![Omit from Alert Rules and Machine Learning](media/machine_learning/image-20210218234536087.png)
